{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31b9b6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "from skdim.id import MLE\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from IntrinsicDim import PHD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66152466",
   "metadata": {},
   "source": [
    "Load the tokenizer and language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c03e624",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at D:/! Models/roberta-base-cased were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "### Insert here path to model files in your syste,\n",
    "model_path = '.../! Models/roberta-base-cased' \n",
    "tokenizer_path = model_path\n",
    "\n",
    "### Loading the model\n",
    "tokenizer = RobertaTokenizer.from_pretrained(tokenizer_path)\n",
    "model = RobertaModel.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedd0f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use this for multilingual model\n",
    "#model_path = '.../xlm-base'\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "#model = AutoModel.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5879e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Our method (PHD) is stochastic, here are some magic constants for it. They are chosen specifically for text data. If you plan to use this code for something different, consider testing other values.\n",
    "\n",
    "MIN_SUBSAMPLE       --- the size of the minimal subsample to be drawn in procedure. Lesser values yields less statisitcally stable predictions.\n",
    "INTERMEDIATE_POINTS --- number of sumsamples to be drawn. The more this number is, the more stable dimension estimation for single text is; however,  the computational time is higher, too. 7 is, empirically, the best trade-off.\n",
    "\"\"\"\n",
    "MIN_SUBSAMPLE = 40 \n",
    "INTERMEDIATE_POINTS = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31a7a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Auxillary function. Clear text from linebreaks and odd whitespaces, because they seem to interfer with LM quite a lot.\n",
    "Replace with a more sophisticated cleaner, if needed.\n",
    "'''\n",
    "\n",
    "def preprocess_text(text):\n",
    "    return text.replace('\\n', ' ').replace('  ', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c64b8c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Get PHD for one text\n",
    "Parameters:\n",
    "        text  --- text\n",
    "        solver --- PHD computator\n",
    "\n",
    "Returns:\n",
    "    real number or NumPy.nan  --- Intrinsic dimension value of the text in the input data\n",
    "                                                    estimated by Persistence Homology Dimension method.'''\n",
    "def get_phd_single(text, solver):\n",
    "    inputs = tokenizer(preprocess_text(text), truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outp = model(**inputs)\n",
    "    \n",
    "    # We omit the first and last tokens (<CLS> and <SEP> because they do not directly correspond to any part of the)\n",
    "    mx_points = inputs['input_ids'].shape[1] - 2\n",
    "\n",
    "    \n",
    "    mn_points = MIN_SUBSAMPLE\n",
    "    step = ( mx_points - mn_points ) // INTERMEDIATE_POINTS\n",
    "        \n",
    "    return solver.fit_transform(outp[0][0].numpy()[1:-1],  min_points=mn_points, max_points=mx_points - step, \\\n",
    "                                point_jump=step)\n",
    "\n",
    "'''\n",
    "Get PHD for all texts in df[key] Pandas DataSeries (PHD method)\n",
    "Parameters:\n",
    "        df  --- Pandas DataFrame\n",
    "        key --- Name of the column\n",
    "        is_list --- Check if the elements of the df[key] are lists (appears in some data)\n",
    "        \n",
    "        alpha --- Parameter alpha for PHD computattion\n",
    "\n",
    "Returns:\n",
    "    numpy.array of shape (number_of_texts, 1) --- Intrinsic dimension values for all texts in the input data\n",
    "                                                    estimated by Persistence Homology Dimension method.\n",
    "'''\n",
    "\n",
    "def get_phd(df, key='text', is_list=False, alpha=1.0):\n",
    "    dims = []\n",
    "    PHD_solver = PHD(alpha=alpha, metric='euclidean', n_points=9)\n",
    "    for s in tqdm(df[key]):\n",
    "        if is_list:\n",
    "            text = s[0]\n",
    "        else:\n",
    "            text = s\n",
    "        dims.append(get_phd_single(text, PHD_solver))\n",
    "\n",
    "    return np.array(dims).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a46c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Get MLE for one text\n",
    "Parameters:\n",
    "        text  --- text\n",
    "        solver --- MLE computator\n",
    "\n",
    "Returns:\n",
    "    real number or NumPy.nan  --- Intrinsic dimension value of the text in the input data\n",
    "                                                    estimated by Maximum Likelihood Estimation method.'''\n",
    "def get_mle_single(text, solver):\n",
    "    inputs = tokenizer(preprocess_text(text), truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outp = model(**inputs)\n",
    "\n",
    "    return solver.fit_transform(outp[0][0].numpy()[1:-1])\n",
    "\n",
    "'''\n",
    "Get PHD for all texts in df[key] Pandas DataSeries (PHD method)\n",
    "Parameters:\n",
    "        df  --- Pandas DataFrame\n",
    "        key --- Name of the column\n",
    "        is_list --- Check if the elements of the df[key] are lists (appears in some data)\n",
    "        \n",
    "Returns:\n",
    "    numpy.array of shape (number_of_texts, 1) --- Intrinsic dimension values for all texts in the input data\n",
    "                                                    estimated by Maximum Likelihood Estimation method.\n",
    "'''\n",
    "\n",
    "def get_mle(df, key='text', is_list=False):\n",
    "    dims = []\n",
    "    MLE_solver = MLE()\n",
    "    for s in tqdm(df[key]):\n",
    "        if is_list:\n",
    "            text = s[0]\n",
    "        else:\n",
    "            text = s\n",
    "        dims.append(get_mle_single(text, MLE_solver))\n",
    "\n",
    "    return np.array(dims).reshape(-1, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07640280",
   "metadata": {},
   "source": [
    "# An example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98dc6ec5",
   "metadata": {},
   "source": [
    "\n",
    "### PHD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "612be5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"Speaking of festivities, there is one day in China that stands unrivaled - the first day of the Lunar New Year, commonly referred to as the Spring Festival. Even if you're generally uninterested in celebratory events, it's hard to resist the allure of the family reunion dinner, a quintessential aspect of the Spring Festival. Throughout the meal, family members raise their glasses to toast one another, expressing wishes for happiness, peace, health, and prosperity in the upcoming year.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "889a7d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PHD estimation of the Intrinsic dimension of sample text is  9.328150478663451\n"
     ]
    }
   ],
   "source": [
    "print(\"PHD estimation of the Intrinsic dimension of sample text is \", get_phd_single(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239bced6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training subset\n",
    "\n",
    "reddit_data = pd.read_json(\"data/Datasets/opt_13b_reddit.jsonl_pp\", lines=True)\n",
    "\n",
    "human_phd_train_en = get_phd(reddit_data.iloc[train_idx], 'gold_completion')\n",
    "opt_phd_train_en = get_phd(reddit_data.iloc[train_idx], 'gen_completion',is_list=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d7ba06",
   "metadata": {},
   "source": [
    "\n",
    "### MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8e8441",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sample_text = \"Speaking of festivities, there is one day in China that stands unrivaled - the first day of the Lunar New Year, commonly referred to as the Spring Festival. Even if you're generally uninterested in celebratory events, it's hard to resist the allure of the family reunion dinner, a quintessential aspect of the Spring Festival. Throughout the meal, family members raise their glasses to toast one another, expressing wishes for happiness, peace, health, and prosperity in the upcoming year.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8209aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MLE estimation of the Intrinsic dimension of sample text is \", get_mle_single(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571044ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training subset\n",
    "\n",
    "reddit_data = pd.read_json(\"data/chatgpt_reddit.jsonl_pp\", lines=True)\n",
    "\n",
    "human_mle_train_en = get_phd(reddit_data.iloc[train_idx], 'gold_completion')\n",
    "chatgpt_mle_train_en = get_phd(reddit_data.iloc[train_idx], 'gen_completion',is_list=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
